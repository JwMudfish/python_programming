{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T03:01:15.687729Z",
     "start_time": "2019-12-04T03:01:15.460726Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import cv2\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenCV - 이미지 읽기, 쓰기 및 표시하기 (1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_image():\n",
    "    imgfile = 'images/sample.png'\n",
    "    # imread_color : 컬러 이미지로 불러오겠다!!\n",
    "    img = cv2.imread(imgfile, cv2.IMREAD_COLOR)\n",
    "    \n",
    "    cv2.imshow('image', img)\n",
    "    cv2.waitKey(0)  # 사용자가 키보드를 누를 때 까지 기둘.. 숫자는 시간을 나타냄, 0은 무한정 기둘..\n",
    "    cv2.destroyAllWindows()  # 모든 윈도우 닫음\n",
    "    cv2.waitKey(1) # 없어도 됨, 주피터에 버그 있어서 이렇게 추가해야 동작함\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    handle_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenCV - 이미지 읽기, 쓰기 및 표시하기 (2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_image():\n",
    "    imgfile = 'images/sample.png'\n",
    "    # imread_greyscale : 흑백 이미지로 불러오겠다!! flag 변화\n",
    "    img = cv2.imread(imgfile, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    cv2.namedWindow('image', cv2.WINDOW_NORMAL)  # 노말 : 원본 이미지 크기, 크기조절 가능\n",
    "    cv2.imshow('image', img)\n",
    "    cv2.waitKey(0)  \n",
    "    \n",
    "    k = cv2.waitKey(0)\n",
    "    \n",
    "    if k == 27:  # 27 - esc 키\n",
    "        cv2.destroyAllWindows()  # 모든 윈도우 닫음\n",
    "        cv2.waitKey(1) # 없어도 됨, 주피터에 버그 있어서 이렇게 추가해야 동작함\n",
    "\n",
    "    elif k == ord('s'):  # s 키 누르면 저장!!\n",
    "        cv2.imwrite('grayImage.png', img)  # (파일경로, 저장할 이미지 객체)\n",
    "        cv2.destroyAllWindows()\n",
    "        cv2.waitKey(1)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    handle_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # OpenCV - 도형 외곽 추출하기 (1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T08:13:43.577332Z",
     "start_time": "2019-10-29T08:13:43.509284Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-16cc77c69e09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mcontour\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-16cc77c69e09>\u001b[0m in \u001b[0;36mcontour\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcontour\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mimgfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'images/contour.jpg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# cvcolor :색공간 추가\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def contour():\n",
    "    imgfile = 'images/contour.jpg'\n",
    "    img = cv2.imread(imgfile)\n",
    "    \n",
    "    # cvcolor :색공간 추가\n",
    "    # bgr2gray :rgb 영역에서 gray컬러로 바꾸겠다.\n",
    "    # 엣지 디텍션은 이미지를 변형시키니까, 복사본 (grey)를 만든다.\n",
    "    imgray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) \n",
    "    \n",
    "    # canny 알고리즘 : 엣지 찾는 것\n",
    "    # 100 해당 값보다 작으면 엣지 아닌 것으로 봄\n",
    "    # 200 해당 값보다 높아야만 엣지로 봄 - 적당한 값을 찾아야 해\n",
    "    edge = cv2.Canny(imgray, 100,200)\n",
    "    \n",
    "    # findcontours : 외곽 찾는 함수, 흰색과 검은색으로 이루어진 이미지를 넘겨야해\n",
    "    # retr_tree : 계층관계를 고려하여 리턴,\n",
    "    # chain_app... : \n",
    "    contours, hierarchy = cv2.findContours(edge, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    cv2.imshow('edge', edge)\n",
    "    # 외곽 그리기 (이미지, 외곽 객체, 컨투어 인덱스(모두 그리고 싶으면 -1), BGR 색상, 선 두께)\n",
    "    cv2.drawContours(img, contours, -1,(0,255,0),1)\n",
    "    cv2.imshow('contour', img)\n",
    "    \n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    contour() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenCV - 도형 외곽 추출하기 (2)\n",
    "- 문서(명함)이 조금 구겨져 있으면?\n",
    "- 정확한 직사각형이 아니면?\n",
    "    - 도형을 근사하여 계산함으로써 overfitting을 방지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contour_approx():\n",
    "    imgfile = 'images/contour2.png'\n",
    "    img = cv2.imread(imgfile)\n",
    "    img2 = img.copy()\n",
    "    imgray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    edge = cv2.Canny(imgray,100,200)\n",
    "    contours, hierarchy = cv2.findContours(edge, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    cnt = contours[0]\n",
    "    cv2.drawContours(img, [cnt], 0, (0,255,0), 3)\n",
    "    \n",
    "    ################img2 (꼭지점을 4개로 근사)###################\n",
    "    # 외곽 둘레길이 계산\n",
    "    # 근사정확도 (오차)\n",
    "    epsilon = 0.1 * cv2.arcLength(cnt, True)\n",
    "    \n",
    "    # 다각형을 대상으로 꼭지점을 줄여나가는 것\n",
    "    # 오차값이 작을수록 원본과 비슷\n",
    "    approx = cv2.approxPolyDP(cnt, epsilon, True)\n",
    "    \n",
    "    cv2.drawContours(img2, [approx], 0, (0,255,0), 3)\n",
    "    \n",
    "    cv2.imshow('Contour', img)\n",
    "    cv2.imshow('Approx', img2)\n",
    "    \n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    contour_approx() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 투영변환 구현하기 (1)\n",
    "- 외곽으로 그려진 영역을 반듯하게 변환하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warp_affine():\n",
    "    img = cv2.imread('images/transform.png')\n",
    "    \n",
    "    # x,y 좌표 이동 (pts1 -> pts2 좌표로)\n",
    "    pts1 = np.float32([[50,50],[200,50],[20,200]])\n",
    "    pts2 = np.float32([[70,100],[220,50],[150,250]])\n",
    "    \n",
    "    # 좌표이동 매트릭스 getaffinetransform\n",
    "    # 이동할 파라메터, 이동 된 파라메동\n",
    "    M = cv2.getAffineTransform(pts1, pts2)\n",
    "    \n",
    "    # 픽셀 좌표이동 warpAffine\n",
    "    result = cv2.warpAffine(img, M, (350,300))\n",
    "    \n",
    "    cv2.imshow('original', img)\n",
    "    cv2.imshow('Affine Transform', result)\n",
    "    \n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    warp_affine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenCV - 투영변환 구현하기 (2)\n",
    "- 포인트를 지정하지 않고 자동으로 반듯하게 하는 방법은?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warp_perspective():\n",
    "    img = cv2.imread('images/transform.jpg')\n",
    "    \n",
    "    #좌표\n",
    "    # 외곽 검출법으로 네개의 꼭지점을 찾았다고 가정\n",
    "    topLeft = [127,157]\n",
    "    topRight = [448, 152]\n",
    "    bottomRight = [579, 526]\n",
    "    bottomLeft = [54, 549]\n",
    "    \n",
    "    pts1 = np.float32([topLeft, topRight, bottomRight, bottomLeft])\n",
    "    \n",
    "    \n",
    "    # 넓이2 높이2 절대값 구하기\n",
    "    w1 = abs(bottomRight[0] - bottomLeft[0])\n",
    "    w2 = abs(topRight[0] - topLeft[0])\n",
    "    h1 = abs(topRight[1] - bottomRight[1])\n",
    "    h2 = abs(topLeft[1] - bottomLeft[1])\n",
    "    \n",
    "    # 최소넓이, 최대 높이\n",
    "    minWidth = min([w1, w2])\n",
    "    minHeight = min([h1, h2])\n",
    "    \n",
    "    # 변환 될 대상 좌표\n",
    "    # 1을 뺀 이유 = 비어있는 픽셀을 만들지 않기 위해서\n",
    "    \n",
    "    pts2 = np.float32([[0,0], [minWidth-1,0], [minWidth-1,minHeight-1], [0,minHeight-1]])\n",
    "    \n",
    "    # getperspective : 좌표기반 픽셀 변환,원근 보정 고려함\n",
    "    # affine : 좌표기반 픽셀 변환, 원근보정 하지 않음\n",
    "    M = cv2.getPerspectiveTransform(pts1, pts2)\n",
    "    \n",
    "    result = cv2.warpPerspective(img, M, (int(minWidth), int(minHeight)))\n",
    "    \n",
    "    cv2.imshow('original', img)\n",
    "    cv2.imshow('Warp Transform', result)\n",
    "    \n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    warp_perspective()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 스캔 한 듯한 효과 주기 (1)\n",
    "- 조명의 영향 제거하기 (글자인식을 위해서)\n",
    "- 모든 색을 0과 1 (흰, 검)로 바꿈 : Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Callback Function for Trackbar (but do not any work)\n",
    "def nothing(x):\n",
    "    pass\n",
    "\n",
    "\n",
    "def global_threshold():\n",
    "    imgfile = 'images/document.jpg'\n",
    "    img = cv2.imread(imgfile, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    # resize image - 연산량 조절을 위해서\n",
    "    r = 600.0 / img.shape[0]   # 이미지 가로 픽셀 600으로 고정\n",
    "    dim = (int(img.shape[1] * r),600)  # 세로 길이\n",
    "    img = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
    "    \n",
    "    WindowName = 'Window'\n",
    "    TrackbarName = 'Threshold'\n",
    "    \n",
    "    # MAke Window and Trackbar\n",
    "    cv2.namedWindow(WindowName)\n",
    "    cv2.createTrackbar(TrackbarName, WindowName, 70, 255, nothing)  # noting 바 이동을 하면 특정 행동 수행 (함수)\n",
    "    \n",
    "    # Allocate destination image\n",
    "    Threshold = np.zeros(img.shape, np.uint8)  #shape : 이미지 가로 세로 픽셀\n",
    "    \n",
    "    # Loop for get trackbar pos and precess it\n",
    "    while True:\n",
    "        #get position in trackbar\n",
    "        TrackbarPos = cv2.getTrackbarPos(TrackbarName, WindowName)\n",
    "        \n",
    "        #Apply threshold\n",
    "        cv2.threshold(img, TrackbarPos, 255, cv2.THRESH_BINARY, Threshold) # 255 threshold 최대값\n",
    "        \n",
    "        # Show in window\n",
    "        cv2.imshow(WindowName, Threshold)\n",
    "        \n",
    "        # wait for ESC key to exit\n",
    "        k = cv2.waitKey(0)\n",
    "        if k == 27:\n",
    "            cv2.destroyAllWindows()\n",
    "            cv2.waitKey(1)\n",
    "            break\n",
    "    return\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    global_threshold() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 스캔한 듯한 효과 주기 - 2\n",
    "- Threshold를 자동으로 구할수는 없나?\n",
    "- 조명의 영향을 조금 더 줄일 수 없을까?\n",
    "- 이미지를 잘개 쪼개서 threshold를 구하자!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCV - 스캔한 듯한 효과 주기 (2)\n",
    "# 배경 외곽 노이즈가 적을수록 모서리 추축하기가 쉬움\n",
    "# blur 효과 때문에 결과적으로 글자는 흐릿해짐\n",
    "# edge 검출 (blur)  ->  꼭지점 저장  ->  blur 안준 이미지를 사용 (글자인식 위해서) \n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def adaptive_threshold():\n",
    "    imgfile = 'images/document.jpg'\n",
    "    img = cv2.imread(imgfile, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    # Resize image\n",
    "    r = 600.0 / img.shape[0]\n",
    "    dim = (int(img.shape[1] * r), 600)\n",
    "    img = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
    "    \n",
    "    # Blur image and apply adaptive threshold\n",
    "    # 2진화 결과를 명확하게 하기 위해서 blur 효과를 준다.\n",
    "    blur = cv2.GaussianBlur(img, (5, 5), 0)   # gaussianblur -> 이미지에 blur 흐리게 함 (이미지 대상, (주변 픽셀), ) 주변픽셀 커질수록 그림이 뭉개짐\n",
    "    result_without_blur = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 21, 10) # 21 - 픽셀 잘게 쪼개는 정도\n",
    "    result_with_blur = cv2.adaptiveThreshold(blur, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 21, 10) # 10 - 주변 영역 평균값에서 빼는 상수값 10이 적당\n",
    "    cv2.imshow('Without Blur', result_without_blur)\n",
    "    cv2.imshow('With Blur', result_with_blur)\n",
    "    \n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    adaptive_threshold() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 명함인식 구현 (각각 단계 합치기!!)\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: Edge Detection\n",
      "STEP 2: Find contours of paper\n",
      "STEP 3: Apply perspective transform\n",
      "STEP 4: Apply Adaptive Threshold\n"
     ]
    }
   ],
   "source": [
    "# 명함인식 구현하기 - 캡처된 이미지\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def order_points(pts):  # 꼭지점 반환\n",
    "    # initialzie a list of coordinates that will be ordered\n",
    "    # such that the first entry in the list is the top-left,\n",
    "    # the second entry is the top-right, the third is the\n",
    "    # bottom-right, and the fourth is the bottom-left\n",
    "    rect = np.zeros((4, 2), dtype = \"float32\")\n",
    "\n",
    "    # the top-left point will have the smallest sum, whereas\n",
    "    # the bottom-right point will have the largest sum\n",
    "    s = pts.sum(axis = 1)\n",
    "    rect[0] = pts[np.argmin(s)]\n",
    "    rect[2] = pts[np.argmax(s)]\n",
    "\n",
    "    # now, compute the difference between the points, the\n",
    "    # top-right point will have the smallest difference,\n",
    "    # whereas the bottom-left will have the largest difference\n",
    "    diff = np.diff(pts, axis = 1)\n",
    "    rect[1] = pts[np.argmin(diff)]\n",
    "    rect[3] = pts[np.argmax(diff)]\n",
    "\n",
    "    # return the ordered coordinates\n",
    "    return rect\n",
    "\n",
    "# edge detection!!\n",
    "def auto_scan_image():\n",
    "    # load the image and compute the ratio of the old height\n",
    "    # to the new height, clone it, and resize it\n",
    "    # document.jpg ~ docuemnt7.jpg\n",
    "    image = cv2.imread('images/document5.jpg')\n",
    "    orig = image.copy()\n",
    "    r = 800.0 / image.shape[0]\n",
    "    dim = (int(image.shape[1] * r), 800)\n",
    "    image = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n",
    "\n",
    "    # convert the image to grayscale, blur it, and find edges\n",
    "    # in the image\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "    edged = cv2.Canny(gray, 75, 200)\n",
    "\n",
    "    # show the original image and the edge detected image\n",
    "    print (\"STEP 1: Edge Detection\")\n",
    "    cv2.imshow(\"Image\", image)\n",
    "    cv2.imshow(\"Edged\", edged)\n",
    "    \n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "    # find the contours in the edged image, keeping only the, 계층관계 쓰지 않음 컨두어만 받음\n",
    "    # largest ones, and initialize the screen contour\n",
    "    (cnts, _) = cv2.findContours(edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = sorted(cnts, key = cv2.contourArea, reverse = True)[:5] # 컨투어 그린 면적을 큰 순서로 5개 가져옴\n",
    "\n",
    "    # loop over the contours\n",
    "    for c in cnts:\n",
    "        # approximate the contour\n",
    "        peri = cv2.arcLength(c, True)  # 컨투어 길이 반환\n",
    "        approx = cv2.approxPolyDP(c, 0.02 * peri, True)  # 길이에 0.02 오차로 근사하여 외곽 추출\n",
    "\n",
    "        # if our approximated contour has four points, then we\n",
    "        # can assume that we have found our screen\n",
    "        if len(approx) == 4:   # 꼭지점이 4개라면, 명함의 외곽이라 봄\n",
    "            screenCnt = approx\n",
    "            break\n",
    "\n",
    "    # show the contour (outline) of the piece of paper\n",
    "    print (\"STEP 2: Find contours of paper\")\n",
    "    cv2.drawContours(image, [screenCnt], -1, (0, 255, 0), 2)   # 컨투어 그림!!\n",
    "    cv2.imshow(\"Outline\", image)\n",
    "    \n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "    # apply the four point transform to obtain a top-down\n",
    "    # view of the original image\n",
    "    # 검출된 외곽으로, 반듯한 사각형 변환!!\n",
    "    rect = order_points(screenCnt.reshape(4, 2) / r)  # 컨투어에서 4개 꼭지점 정렬 함수\n",
    "    (topLeft, topRight, bottomRight, bottomLeft) = rect\n",
    "    \n",
    "    w1 = abs(bottomRight[0] - bottomLeft[0])\n",
    "    w2 = abs(topRight[0] - topLeft[0])\n",
    "    h1 = abs(topRight[1] - bottomRight[1])\n",
    "    h2 = abs(topLeft[1] - bottomLeft[1])\n",
    "    maxWidth = max([w1, w2])\n",
    "    maxHeight = max([h1, h2])\n",
    "    \n",
    "    dst = np.float32([[0,0], [maxWidth-1,0], \n",
    "                      [maxWidth-1,maxHeight-1], [0,maxHeight-1]])\n",
    "    \n",
    "    M = cv2.getPerspectiveTransform(rect, dst)\n",
    "    warped = cv2.warpPerspective(orig, M, (maxWidth, maxHeight))\n",
    "\n",
    "    # show the original and scanned images\n",
    "    print (\"STEP 3: Apply perspective transform\")\n",
    "    cv2.imshow(\"Warped\", warped)\n",
    "    \n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "    \n",
    "    # convert the warped image to grayscale, then threshold it\n",
    "    # to give it that 'black and white' paper effect\n",
    "    warped = cv2.cvtColor(warped, cv2.COLOR_BGR2GRAY)\n",
    "    warped = cv2.adaptiveThreshold(warped, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 21, 10)\n",
    "\n",
    "    # show the original and scanned images\n",
    "    print (\"STEP 4: Apply Adaptive Threshold\")\n",
    "    cv2.imshow(\"Original\", orig)\n",
    "    cv2.imshow(\"Scanned\", warped)\n",
    "    cv2.imwrite('scannedImage.png', warped)\n",
    "    \n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    auto_scan_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 명함인식 구현하기 - 웹캠 (1)\n",
    "- 카메라에서 실시간으로 명함 인식.\n",
    "- 루프를 돌면서 카메라에서 프레임을 추출한 후 알고리즘 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T03:03:26.524624Z",
     "start_time": "2019-12-04T03:03:26.499300Z"
    }
   },
   "outputs": [],
   "source": [
    "# 명함인식 구현하기 - 웹캠(1)\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def auto_scan_image_via_webcam():\n",
    "    \n",
    "    try: \n",
    "        cap = cv2.VideoCapture(0)\n",
    "    except:\n",
    "        print ('cannot load camera!')\n",
    "        return\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print ('cannot load camera!')\n",
    "            break\n",
    "            \n",
    "        k = cv2.waitKey(10)\n",
    "        if k == 27:\n",
    "            break\n",
    "        \n",
    "        # convert the image to grayscale, blur it, and find edges\n",
    "        # in the image\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        gray = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "        edged = cv2.Canny(gray, 75, 200)\n",
    "\n",
    "        # show the original image and the edge detected image\n",
    "        print (\"STEP 1: Edge Detection\")\n",
    "\n",
    "        # find the contours in the edged image, keeping only the\n",
    "        # largest ones, and initialize the screen contour\n",
    "        (_, cnts, _) = cv2.findContours(edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cnts = sorted(cnts, key = cv2.contourArea, reverse = True)[:5]\n",
    "\n",
    "        # loop over the contours\n",
    "        for c in cnts:\n",
    "            # approximate the contour\n",
    "            peri = cv2.arcLength(c, True)\n",
    "            approx = cv2.approxPolyDP(c, 0.02 * peri, True)\n",
    "            screenCnt = []\n",
    "\n",
    "            # if our approximated contour has four points, then we\n",
    "            # can assume that we have found our screen\n",
    "            # 불필요한 사각형을 검출하지 않는 로직!! 전체화면의 10% 이상 차지할 때만 사각형 검출\n",
    "            if len(approx) == 4:\n",
    "                contourSize = cv2.contourArea(approx)\n",
    "                camSize = frame.shape[0] * frame.shape[1]       \n",
    "                ratio = contourSize / camSize\n",
    "                print (contourSize)\n",
    "                print (camSize)\n",
    "                print (ratio)\n",
    "                \n",
    "                if ratio > 0.1:\n",
    "                    screenCnt = approx\n",
    "                    \n",
    "                break \n",
    "        \n",
    "        if len(screenCnt) == 0:\n",
    "            cv2.imshow(\"WebCam\", frame)\n",
    "            continue\n",
    "            \n",
    "        else:\n",
    "            # show the contour (outline) of the piece of paper\n",
    "            print (\"STEP 2: Find contours of paper\")\n",
    "\n",
    "            cv2.drawContours(frame, [screenCnt], -1, (0, 255, 0), 2)\n",
    "            cv2.imshow(\"WebCam\", frame)\n",
    "        \n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    auto_scan_image_via_webcam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 웹캠에서 검출 - 스캔효과까지.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T03:03:35.743508Z",
     "start_time": "2019-12-04T03:03:35.699074Z"
    }
   },
   "outputs": [],
   "source": [
    "# 명함인식 구현하기 - 웹캠(2)\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def order_points(pts):\n",
    "    # initialzie a list of coordinates that will be ordered\n",
    "    # such that the first entry in the list is the top-left,\n",
    "    # the second entry is the top-right, the third is the\n",
    "    # bottom-right, and the fourth is the bottom-left\n",
    "    rect = np.zeros((4, 2), dtype = \"float32\")\n",
    "\n",
    "    # the top-left point will have the smallest sum, whereas\n",
    "    # the bottom-right point will have the largest sum\n",
    "    s = pts.sum(axis = 1)\n",
    "    rect[0] = pts[np.argmin(s)]\n",
    "    rect[2] = pts[np.argmax(s)]\n",
    "\n",
    "    # now, compute the difference between the points, the\n",
    "    # top-right point will have the smallest difference,\n",
    "    # whereas the bottom-left will have the largest difference\n",
    "    diff = np.diff(pts, axis = 1)\n",
    "    rect[1] = pts[np.argmin(diff)]\n",
    "    rect[3] = pts[np.argmax(diff)]\n",
    "\n",
    "    # return the ordered coordinates\n",
    "    return rect\n",
    "\n",
    "def auto_scan_image_via_webcam():\n",
    "    \n",
    "    try: \n",
    "        cap = cv2.VideoCapture(0)\n",
    "    except:\n",
    "        print ('cannot load camera!')\n",
    "        return\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print ('cannot load camera!')\n",
    "            break\n",
    "            \n",
    "        k = cv2.waitKey(10)\n",
    "        if k == 27:\n",
    "            break\n",
    "\n",
    "        # convert the image to grayscale, blur it, and find edges\n",
    "        # in the image\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        gray = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "        edged = cv2.Canny(gray, 75, 200)\n",
    "\n",
    "        # show the original image and the edge detected image\n",
    "        print (\"STEP 1: Edge Detection\")\n",
    "\n",
    "        # find the contours in the edged image, keeping only the\n",
    "        # largest ones, and initialize the screen contour\n",
    "        #(_, cnts, _) = cv2.findContours(edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        (cnts, _) = cv2.findContours(edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cnts = sorted(cnts, key = cv2.contourArea, reverse = True)[:5]\n",
    "\n",
    "        # loop over the contours\n",
    "        for c in cnts:\n",
    "            # approximate the contour\n",
    "            peri = cv2.arcLength(c, True)\n",
    "            approx = cv2.approxPolyDP(c, 0.02 * peri, True)\n",
    "            screenCnt = []\n",
    "\n",
    "            # if our approximated contour has four points, then we\n",
    "            # can assume that we have found our screen\n",
    "            if len(approx) == 4:\n",
    "                contourSize = cv2.contourArea(approx)\n",
    "                camSize = frame.shape[0] * frame.shape[1]\n",
    "                ratio = contourSize / camSize\n",
    "                print (contourSize)\n",
    "                print (camSize)\n",
    "                print (ratio)\n",
    "                \n",
    "                if ratio > 0.1:\n",
    "                    screenCnt = approx\n",
    "                    \n",
    "                break \n",
    "        \n",
    "        if len(screenCnt) == 0:\n",
    "            cv2.imshow(\"WebCam\", frame)\n",
    "            continue\n",
    "            \n",
    "        else:\n",
    "            # show the contour (outline) of the piece of paper\n",
    "            print (\"STEP 2: Find contours of paper\")\n",
    "\n",
    "            cv2.drawContours(frame, [screenCnt], -1, (0, 255, 0), 2)\n",
    "            cv2.imshow(\"WebCam\", frame)\n",
    "            \n",
    "            # apply the four point transform to obtain a top-down\n",
    "            # view of the original image\n",
    "            rect = order_points(screenCnt.reshape(4, 2))\n",
    "            (topLeft, topRight, bottomRight, bottomLeft) = rect\n",
    "\n",
    "            w1 = abs(bottomRight[0] - bottomLeft[0])\n",
    "            w2 = abs(topRight[0] - topLeft[0])\n",
    "            h1 = abs(topRight[1] - bottomRight[1])\n",
    "            h2 = abs(topLeft[1] - bottomLeft[1])\n",
    "            maxWidth = max([w1, w2])\n",
    "            maxHeight = max([h1, h2])\n",
    "\n",
    "            dst = np.float32([[0,0], [maxWidth-1,0], \n",
    "                              [maxWidth-1,maxHeight-1], [0,maxHeight-1]])\n",
    "\n",
    "            M = cv2.getPerspectiveTransform(rect, dst)\n",
    "            warped = cv2.warpPerspective(frame, M, (maxWidth, maxHeight))\n",
    "\n",
    "            # show the original and scanned images\n",
    "            print (\"STEP 3: Apply perspective transform\")\n",
    "\n",
    "            # convert the warped image to grayscale, then threshold it\n",
    "            # to give it that 'black and white' paper effect\n",
    "            warped = cv2.cvtColor(warped, cv2.COLOR_BGR2GRAY)\n",
    "            warped = cv2.adaptiveThreshold(warped, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 21, 10)\n",
    "\n",
    "            # show the original and scanned images\n",
    "            print (\"STEP 4: Apply Adaptive Threshold\")\n",
    "\n",
    "            break\n",
    "        \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "    \n",
    "    cv2.imshow(\"Scanned\", warped)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "        \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    auto_scan_image_via_webcam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OCR - Tesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vpstg OAR O\n",
      "Special Coupon\n",
      "\n",
      "“ PARKSEUNGCHOL\n",
      "HAIRSTUDIO\n"
     ]
    }
   ],
   "source": [
    "# OCR - Tesseract\n",
    "\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "\n",
    "def ocr_tesseract():\n",
    "    image_file = 'images/scannedImage.png'\n",
    "    im = Image.open(image_file)\n",
    "    text = pytesseract.image_to_string(im)\n",
    "    im.show()\n",
    "\n",
    "    print (text)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ocr_tesseract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OCR - Project Oxford by MS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OCR - Project Oxford by MS\n",
    "\n",
    "from PIL import Image\n",
    "import http.client, urllib.request, urllib.parse, urllib.error, base64, json\n",
    "\n",
    "def print_text(json_data):\n",
    "    result = json.loads(json_data)\n",
    "    for l in result['regions']:\n",
    "        for w in l['lines']:\n",
    "            line = []\n",
    "            for r in w['words']:\n",
    "                line.append(r['text'])\n",
    "            print (' '.join(line))\n",
    "    return\n",
    "\n",
    "def ocr_project_oxford(headers, params, data):\n",
    "    conn = http.client.HTTPSConnection('westus.api.cognitive.microsoft.com')\n",
    "    conn.request(\"POST\", \"/vision/v1.0/ocr?%s\" % params, data, headers)\n",
    "    response = conn.getresponse()\n",
    "    data = response.read().decode()\n",
    "    print (data + \"\\n\")\n",
    "    print_text(data)\n",
    "    conn.close()\n",
    "    return\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    headers = {\n",
    "        # Request headers\n",
    "        'Content-Type': 'application/octet-stream',\n",
    "        'Ocp-Apim-Subscription-Key': '',  # free key 입력!!!\n",
    "    }\n",
    "    params = urllib.parse.urlencode({\n",
    "        # Request parameters\n",
    "        'language': 'unk',  # ko  한글 인식 가능하게..\n",
    "        'detectOrientation ': 'true',\n",
    "    })\n",
    "    data = open('images/scannedImage.png', 'rb').read()\n",
    "    \n",
    "    try:\n",
    "        image_file = 'images/scannedImage.png'\n",
    "        im = Image.open(image_file)\n",
    "        im.show()\n",
    "        ocr_project_oxford(headers, params, data)\n",
    "    except Exception as e:\n",
    "        print (e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최종 명함인식 - 웹캠 + OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 2: Find contours of paper\n",
      "STEP 3: Apply perspective transform\n",
      "STEP 4: Apply Adaptive Threshold\n",
      "{\"error\":{\"code\":\"401\",\"message\":\"Access denied due to invalid subscription key or wrong API endpoint. Make sure to provide a valid key for an active subscription and use a correct regional API endpoint for your resource.\"}}\n",
      "\n",
      "'regions'\n"
     ]
    }
   ],
   "source": [
    "# 명함인식 구현하기 - 웹캠 + OCR\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import http.client, urllib.request, urllib.parse, urllib.error, base64, json\n",
    "\n",
    "def print_text(json_data):\n",
    "    result = json.loads(json_data)\n",
    "    for l in result['regions']:\n",
    "        for w in l['lines']:\n",
    "            line = []\n",
    "            for r in w['words']:\n",
    "                line.append(r['text'])\n",
    "            print (' '.join(line))\n",
    "    return\n",
    "\n",
    "def ocr_project_oxford(headers, params, data):\n",
    "    conn = http.client.HTTPSConnection('westus.api.cognitive.microsoft.com')\n",
    "    conn.request(\"POST\", \"/vision/v1.0/ocr?%s\" % params, data, headers)\n",
    "    response = conn.getresponse()\n",
    "    data = response.read().decode()\n",
    "    print (data + \"\\n\")\n",
    "    print_text(data)\n",
    "    conn.close()\n",
    "    return\n",
    "\n",
    "def order_points(pts):\n",
    "    # initialzie a list of coordinates that will be ordered\n",
    "    # such that the first entry in the list is the top-left,\n",
    "    # the second entry is the top-right, the third is the\n",
    "    # bottom-right, and the fourth is the bottom-left\n",
    "    rect = np.zeros((4, 2), dtype = \"float32\")\n",
    "\n",
    "    # the top-left point will have the smallest sum, whereas\n",
    "    # the bottom-right point will have the largest sum\n",
    "    s = pts.sum(axis = 1)\n",
    "    rect[0] = pts[np.argmin(s)]\n",
    "    rect[2] = pts[np.argmax(s)]\n",
    "\n",
    "    # now, compute the difference between the points, the\n",
    "    # top-right point will have the smallest difference,\n",
    "    # whereas the bottom-left will have the largest difference\n",
    "    diff = np.diff(pts, axis = 1)\n",
    "    rect[1] = pts[np.argmin(diff)]\n",
    "    rect[3] = pts[np.argmax(diff)]\n",
    "\n",
    "    # return the ordered coordinates\n",
    "    return rect\n",
    "\n",
    "def auto_scan_image_via_webcam():\n",
    "    \n",
    "    try: \n",
    "        cap = cv2.VideoCapture(0)\n",
    "    except:\n",
    "        print ('cannot load camera')\n",
    "        return\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print ('cannot load camera!')\n",
    "            break\n",
    "            \n",
    "        k = cv2.waitKey(10)\n",
    "        if k == 27:\n",
    "            break\n",
    "\n",
    "        # convert the image to grayscale, blur it, and find edges\n",
    "        # in the image\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        gray = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "        edged = cv2.Canny(gray, 75, 200)\n",
    "\n",
    "        # show the original image and the edge detected image\n",
    "        # print (\"STEP 1: Edge Detection\")\n",
    "\n",
    "        # find the contours in the edged image, keeping only the\n",
    "        # largest ones, and initialize the screen contour\n",
    "        #(_, cnts, _) = cv2.findContours(edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        (cnts, _) = cv2.findContours(edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cnts = sorted(cnts, key = cv2.contourArea, reverse = True)[:5]\n",
    "\n",
    "        # loop over the contours\n",
    "        for c in cnts:\n",
    "            # approximate the contour\n",
    "            peri = cv2.arcLength(c, True)\n",
    "            approx = cv2.approxPolyDP(c, 0.02 * peri, True)\n",
    "            screenCnt = []\n",
    "\n",
    "            # if our approximated contour has four points, then we\n",
    "            # can assume that we have found our screen\n",
    "            if len(approx) == 4:\n",
    "                contourSize = cv2.contourArea(approx)\n",
    "                camSize = frame.shape[0] * frame.shape[1]\n",
    "                ratio = contourSize / camSize\n",
    "                # print (contourSize)\n",
    "                # print (camSize)\n",
    "                # print (ratio)\n",
    "                \n",
    "                if ratio > 0.1:\n",
    "                    screenCnt = approx\n",
    "                    \n",
    "                break \n",
    "        \n",
    "        if len(screenCnt) == 0:\n",
    "            cv2.imshow(\"WebCam\", frame)\n",
    "            continue\n",
    "            \n",
    "        else:\n",
    "            # show the contour (outline) of the piece of paper\n",
    "            print (\"STEP 2: Find contours of paper\")\n",
    "\n",
    "            cv2.drawContours(frame, [screenCnt], -1, (0, 255, 0), 2)\n",
    "            cv2.imshow(\"WebCam\", frame)\n",
    "            \n",
    "            # apply the four point transform to obtain a top-down\n",
    "            # view of the original image\n",
    "            rect = order_points(screenCnt.reshape(4, 2))\n",
    "            (topLeft, topRight, bottomRight, bottomLeft) = rect\n",
    "\n",
    "            w1 = abs(bottomRight[0] - bottomLeft[0])\n",
    "            w2 = abs(topRight[0] - topLeft[0])\n",
    "            h1 = abs(topRight[1] - bottomRight[1])\n",
    "            h2 = abs(topLeft[1] - bottomLeft[1])\n",
    "            maxWidth = max([w1, w2])\n",
    "            maxHeight = max([h1, h2])\n",
    "\n",
    "            dst = np.float32([[0,0], [maxWidth-1,0], \n",
    "                              [maxWidth-1,maxHeight-1], [0,maxHeight-1]])\n",
    "\n",
    "            M = cv2.getPerspectiveTransform(rect, dst)\n",
    "            warped = cv2.warpPerspective(frame, M, (maxWidth, maxHeight))\n",
    "\n",
    "            # show the original and scanned images\n",
    "            print (\"STEP 3: Apply perspective transform\")\n",
    "\n",
    "            # convert the warped image to grayscale, then threshold it\n",
    "            # to give it that 'black and white' paper effect\n",
    "            warped = cv2.cvtColor(warped, cv2.COLOR_BGR2GRAY)\n",
    "            warped = cv2.adaptiveThreshold(warped, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 21, 10)\n",
    "\n",
    "            # show the original and scanned images\n",
    "            print (\"STEP 4: Apply Adaptive Threshold\")\n",
    "\n",
    "            break\n",
    "        \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "    \n",
    "    cv2.imshow(\"Scanned\", warped)\n",
    "    cv2.imwrite('scannedImage.png', warped)\n",
    "    \n",
    "    headers = {\n",
    "        # Request headers\n",
    "        'Content-Type': 'application/octet-stream',\n",
    "        'Ocp-Apim-Subscription-Key': '',\n",
    "    }\n",
    "    params = urllib.parse.urlencode({\n",
    "        # Request parameters\n",
    "        'language': 'unk',\n",
    "        'detectOrientation ': 'true',\n",
    "    })\n",
    "    data = open('scannedImage.png', 'rb').read()\n",
    "    \n",
    "    try:\n",
    "        image_file = 'scannedImage.png'\n",
    "        ocr_project_oxford(headers, params, data)\n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "        \n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "        \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    auto_scan_image_via_webcam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenCV - 이미지에서 텍스트 영역만 찾아내기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images/scannedImage.png not enough values to unpack (expected 3, got 2)\n"
     ]
    }
   ],
   "source": [
    "# (참고) OpenCV - 이미지에서 텍스트 영역만 찾아내기\n",
    "\n",
    "# 출처: http://www.danvk.org/2015/01/07/finding-blocks-of-text-in-an-image-using-python-opencv-and-numpy.html\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "from scipy.ndimage.filters import rank_filter\n",
    "\n",
    "\n",
    "def dilate(ary, N, iterations):\n",
    "    \"\"\"Dilate using an NxN '+' sign shape. ary is np.uint8.\"\"\"\n",
    "    kernel = np.zeros((N,N), dtype=np.uint8)\n",
    "    kernel[(N-1)/2,:] = 1\n",
    "    dilated_image = cv2.dilate(ary / 255, kernel, iterations=iterations)\n",
    "\n",
    "    kernel = np.zeros((N,N), dtype=np.uint8)\n",
    "    kernel[:,(N-1)/2] = 1\n",
    "    dilated_image = cv2.dilate(dilated_image, kernel, iterations=iterations)\n",
    "    dilated_image = cv2.convertScaleAbs(dilated_image)\n",
    "    return dilated_image\n",
    "\n",
    "\n",
    "def props_for_contours(contours, ary):\n",
    "    \"\"\"Calculate bounding box & the number of set pixels for each contour.\"\"\"\n",
    "    c_info = []\n",
    "    for c in contours:\n",
    "        x,y,w,h = cv2.boundingRect(c)\n",
    "        c_im = np.zeros(ary.shape)\n",
    "        cv2.drawContours(c_im, [c], 0, 255, -1)\n",
    "        c_info.append({\n",
    "            'x1': x,\n",
    "            'y1': y,\n",
    "            'x2': x + w - 1,\n",
    "            'y2': y + h - 1,\n",
    "            'sum': np.sum(ary * (c_im > 0))/255\n",
    "        })\n",
    "    return c_info\n",
    "\n",
    "\n",
    "def union_crops(crop1, crop2):\n",
    "    \"\"\"Union two (x1, y1, x2, y2) rects.\"\"\"\n",
    "    x11, y11, x21, y21 = crop1\n",
    "    x12, y12, x22, y22 = crop2\n",
    "    return min(x11, x12), min(y11, y12), max(x21, x22), max(y21, y22)\n",
    "\n",
    "\n",
    "def intersect_crops(crop1, crop2):\n",
    "    x11, y11, x21, y21 = crop1\n",
    "    x12, y12, x22, y22 = crop2\n",
    "    return max(x11, x12), max(y11, y12), min(x21, x22), min(y21, y22)\n",
    "\n",
    "\n",
    "def crop_area(crop):\n",
    "    x1, y1, x2, y2 = crop\n",
    "    return max(0, x2 - x1) * max(0, y2 - y1)\n",
    "\n",
    "\n",
    "def find_border_components(contours, ary):\n",
    "    borders = []\n",
    "    area = ary.shape[0] * ary.shape[1]\n",
    "    for i, c in enumerate(contours):\n",
    "        x,y,w,h = cv2.boundingRect(c)\n",
    "        if w * h > 0.5 * area:\n",
    "            borders.append((i, x, y, x + w - 1, y + h - 1))\n",
    "    return borders\n",
    "\n",
    "\n",
    "def angle_from_right(deg):\n",
    "    return min(deg % 90, 90 - (deg % 90))\n",
    "\n",
    "\n",
    "def remove_border(contour, ary):\n",
    "    \"\"\"Remove everything outside a border contour.\"\"\"\n",
    "    # Use a rotated rectangle (should be a good approximation of a border).\n",
    "    # If it's far from a right angle, it's probably two sides of a border and\n",
    "    # we should use the bounding box instead.\n",
    "    c_im = np.zeros(ary.shape)\n",
    "    r = cv2.minAreaRect(contour)\n",
    "    degs = r[2]\n",
    "    if angle_from_right(degs) <= 10.0:\n",
    "        box = cv2.boxPoints(r)\n",
    "        box = np.int0(box)\n",
    "        cv2.drawContours(c_im, [box], 0, 255, -1)\n",
    "        cv2.drawContours(c_im, [box], 0, 0, 4)\n",
    "    else:\n",
    "        x1, y1, x2, y2 = cv2.boundingRect(contour)\n",
    "        cv2.rectangle(c_im, (x1, y1), (x2, y2), 255, -1)\n",
    "        cv2.rectangle(c_im, (x1, y1), (x2, y2), 0, 4)\n",
    "\n",
    "    return np.minimum(c_im, ary)\n",
    "\n",
    "\n",
    "def find_components(edges, max_components=16):\n",
    "    \"\"\"Dilate the image until there are just a few connected components.\n",
    "    Returns contours for these components.\"\"\"\n",
    "    # Perform increasingly aggressive dilation until there are just a few\n",
    "    # connected components.\n",
    "    count = 21\n",
    "    dilation = 5\n",
    "    n = 1\n",
    "    while count > 16:\n",
    "        n += 1\n",
    "        dilated_image = dilate(edges, N=3, iterations=n)\n",
    "        _, contours, hierarchy = cv2.findContours(dilated_image, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        count = len(contours)\n",
    "    #print dilation\n",
    "    #Image.fromarray(edges).show()\n",
    "    #Image.fromarray(255 * dilated_image).show()\n",
    "    return contours\n",
    "\n",
    "\n",
    "def find_optimal_components_subset(contours, edges):\n",
    "    \"\"\"Find a crop which strikes a good balance of coverage/compactness.\n",
    "    Returns an (x1, y1, x2, y2) tuple.\n",
    "    \"\"\"\n",
    "    c_info = props_for_contours(contours, edges)\n",
    "    c_info.sort(key=lambda x: -x['sum'])\n",
    "    total = np.sum(edges) / 255\n",
    "    area = edges.shape[0] * edges.shape[1]\n",
    "\n",
    "    c = c_info[0]\n",
    "    del c_info[0]\n",
    "    this_crop = c['x1'], c['y1'], c['x2'], c['y2']\n",
    "    crop = this_crop\n",
    "    covered_sum = c['sum']\n",
    "\n",
    "    while covered_sum < total:\n",
    "        changed = False\n",
    "        recall = 1.0 * covered_sum / total\n",
    "        prec = 1 - 1.0 * crop_area(crop) / area\n",
    "        f1 = 2 * (prec * recall / (prec + recall))\n",
    "        #print '----'\n",
    "        for i, c in enumerate(c_info):\n",
    "            this_crop = c['x1'], c['y1'], c['x2'], c['y2']\n",
    "            new_crop = union_crops(crop, this_crop)\n",
    "            new_sum = covered_sum + c['sum']\n",
    "            new_recall = 1.0 * new_sum / total\n",
    "            new_prec = 1 - 1.0 * crop_area(new_crop) / area\n",
    "            new_f1 = 2 * new_prec * new_recall / (new_prec + new_recall)\n",
    "\n",
    "            # Add this crop if it improves f1 score,\n",
    "            # _or_ it adds 25% of the remaining pixels for <15% crop expansion.\n",
    "            # ^^^ very ad-hoc! make this smoother\n",
    "            remaining_frac = c['sum'] / (total - covered_sum)\n",
    "            new_area_frac = 1.0 * crop_area(new_crop) / crop_area(crop) - 1\n",
    "            if new_f1 > f1 or (\n",
    "                    remaining_frac > 0.25 and new_area_frac < 0.15):\n",
    "                print('%d %s -> %s / %s (%s), %s -> %s / %s (%s), %s -> %s' % (\n",
    "                        i, covered_sum, new_sum, total, remaining_frac,\n",
    "                        crop_area(crop), crop_area(new_crop), area, new_area_frac,\n",
    "                        f1, new_f1))\n",
    "                crop = new_crop\n",
    "                covered_sum = new_sum\n",
    "                del c_info[i]\n",
    "                changed = True\n",
    "                break\n",
    "\n",
    "        if not changed:\n",
    "            break\n",
    "\n",
    "    return crop\n",
    "\n",
    "\n",
    "def pad_crop(crop, contours, edges, border_contour, pad_px=15):\n",
    "    \"\"\"Slightly expand the crop to get full contours.\n",
    "    This will expand to include any contours it currently intersects, but will\n",
    "    not expand past a border.\n",
    "    \"\"\"\n",
    "    bx1, by1, bx2, by2 = 0, 0, edges.shape[0], edges.shape[1]\n",
    "    if border_contour is not None and len(border_contour) > 0:\n",
    "        c = props_for_contours([border_contour], edges)[0]\n",
    "        bx1, by1, bx2, by2 = c['x1'] + 5, c['y1'] + 5, c['x2'] - 5, c['y2'] - 5\n",
    "\n",
    "    def crop_in_border(crop):\n",
    "        x1, y1, x2, y2 = crop\n",
    "        x1 = max(x1 - pad_px, bx1)\n",
    "        y1 = max(y1 - pad_px, by1)\n",
    "        x2 = min(x2 + pad_px, bx2)\n",
    "        y2 = min(y2 + pad_px, by2)\n",
    "        return crop\n",
    "\n",
    "    crop = crop_in_border(crop)\n",
    "\n",
    "    c_info = props_for_contours(contours, edges)\n",
    "    changed = False\n",
    "    for c in c_info:\n",
    "        this_crop = c['x1'], c['y1'], c['x2'], c['y2']\n",
    "        this_area = crop_area(this_crop)\n",
    "        int_area = crop_area(intersect_crops(crop, this_crop))\n",
    "        new_crop = crop_in_border(union_crops(crop, this_crop))\n",
    "        if 0 < int_area < this_area and crop != new_crop:\n",
    "            print('%s -> %s' % (str(crop), str(new_crop)))\n",
    "            changed = True\n",
    "            crop = new_crop\n",
    "\n",
    "    if changed:\n",
    "        return pad_crop(crop, contours, edges, border_contour, pad_px)\n",
    "    else:\n",
    "        return crop\n",
    "\n",
    "\n",
    "def downscale_image(im, max_dim=2048):\n",
    "    \"\"\"Shrink im until its longest dimension is <= max_dim.\n",
    "    Returns new_image, scale (where scale <= 1).\n",
    "    \"\"\"\n",
    "    a = im.shape[0]\n",
    "    b = im.shape[1]\n",
    "    if max(a, b) <= max_dim:\n",
    "        return 1.0, im\n",
    "\n",
    "    scale = 1.0 * max_dim / max(a, b)\n",
    "    dim = (int(a * scale), int(b * scale))\n",
    "    new_im = cv2.resize(im, dim, interpolation = cv2.INTER_AREA)\n",
    "    \n",
    "    return scale, new_im\n",
    "\n",
    "\n",
    "def process_image(path, out_path):\n",
    "    orig_im = Image.open(path)\n",
    "    im = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    scale, im = downscale_image(im)\n",
    "\n",
    "    edges = cv2.Canny(im, 100, 200)\n",
    "\n",
    "    # TODO: dilate image _before_ finding a border. This is crazy sensitive!\n",
    "    _, contours, hierarchy = cv2.findContours(edges, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    borders = find_border_components(contours, edges)\n",
    "    borders.sort(key=lambda i_x1_y1_x2_y2: (i_x1_y1_x2_y2[3] - i_x1_y1_x2_y2[1]) * (i_x1_y1_x2_y2[4] - i_x1_y1_x2_y2[2]))\n",
    "\n",
    "    border_contour = None\n",
    "    if len(borders):\n",
    "        border_contour = contours[borders[0][0]]\n",
    "        edges = remove_border(border_contour, edges)\n",
    "\n",
    "    edges = 255 * (edges > 0).astype(np.uint8)\n",
    "\n",
    "    # Remove ~1px borders using a rank filter.\n",
    "    maxed_rows = rank_filter(edges, -5, size=(1, 20))\n",
    "    maxed_cols = rank_filter(edges, -5, size=(20, 1))\n",
    "    debordered = np.minimum(np.minimum(edges, maxed_rows), maxed_cols)\n",
    "    edges = debordered\n",
    "\n",
    "    contours = find_components(edges)\n",
    "    if len(contours) == 0:\n",
    "        print('%s -> (no text!)' % path)\n",
    "        return\n",
    "\n",
    "    crop = find_optimal_components_subset(contours, edges)\n",
    "    crop = pad_crop(crop, contours, edges, border_contour)\n",
    "\n",
    "    crop = [int(x / scale) for x in crop]  # upscale to the original image size.\n",
    "\n",
    "    # draw and show cropped rectangle area in the original image\n",
    "    rgb_im = orig_im.convert('RGB')\n",
    "    draw = ImageDraw.Draw(rgb_im)\n",
    "    draw.rectangle(crop, outline='red')\n",
    "    rgb_im.show()\n",
    "\n",
    "    text_im = orig_im.crop(crop)\n",
    "    text_im.show()\n",
    "    text_im.save(out_path)\n",
    "    print('%s -> %s' % (path, out_path))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # path = 'images/text.jpg'\n",
    "    path = 'images/scannedImage.png'\n",
    "    out_path = 'croppedImage.png'\n",
    "    try:\n",
    "        process_image(path, out_path)\n",
    "    except Exception as e:\n",
    "        print('%s %s' % (path, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
